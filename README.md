# ðŸ§  RAG Practice â€“ Building a Retrieval-Augmented Generation Pipeline from Scratch

This project is my personal attempt to **understand how Retrieval-Augmented Generation (RAG)** works by implementing each component from the ground up.

The pipeline was built in Python, and the main goal was **learning by doing** â€” not just using high-level libraries like LangChain, but actually wiring up each stage myself.

---

## ðŸš€ What I Built

- âœ… Parsed and chunked Wikipedia-style text (from `wikitext-103`)
- âœ… Generated dense vector embeddings using `sentence-transformers`
- âœ… Indexed document chunks in FAISS for fast retrieval
- âœ… Retrieved relevant chunks based on a user question
- âœ… Used an LLM (e.g. Mistral-7B or FLAN-T5) to generate an answer from retrieved content
- âœ… Ran everything locally â€” including trying GPU inference

---

## ðŸŽ¯ What I Wanted to Learn

- What RAG *really* is under the hood
- How to connect a retriever (FAISS) with a generator (transformers)
- How embeddings work and how to chunk documents effectively
- How GPU acceleration affects inference
- How to clean large files and secrets from Git history ðŸ˜…

---

## ðŸ›  Stack

- Python
- Hugging Face Transformers
- SentenceTransformers
- FAISS
- Optional: bitsandbytes + CUDA
- Dataset: `wikitext-103` from Hugging Face ðŸ¤“

---

## ðŸ§ª Current Interface

Everything runs from the terminal with:

```bash
python rag_app.py
